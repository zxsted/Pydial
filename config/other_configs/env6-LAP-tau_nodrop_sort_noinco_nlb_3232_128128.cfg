# Error model: 30% error rate, DSTC2 confscorer, DSTC2 nbestgenerator
# User model: standard sampled params, sampled patience
# Masks: on

###### General parameters ######
[GENERAL]
domains = Laptops11
singledomain = True 
tracedialog = 0
seed = 07051991

[exec_config]
configdir = _env6_madqn_tau_nodrop_sort_noinco_nlb_3232_128128_policies
logfiledir = _env6_madqn_tau_nodrop_sort_noinco_nlb_3232_128128_logs
numtrainbatches = 4
traindialogsperbatch = 1000
numbatchtestdialogs =  500
trainsourceiteration = 0
numtestdialogs =  500
trainerrorrate = 30
testerrorrate  = 30
testeverybatch = True
#deleteprevpolicy = True

[logging]
usecolor = False
screen_level = results
file_level = results
file = auto

###### Environment parameters ######

[agent]
maxturns = 25

[usermodel]
usenewgoalscenarios = True
oldstylepatience = False
patience = 4,6
configfile = config/sampledUM.cfg

[errormodel]
nbestsize = 5
confusionmodel = LevenshteinConfusions
nbestgeneratormodel = DSTC2NBestGenerator
confscorer = DSTC2
configfile = config/set3-ErrorModel.cfg


[summaryacts]
maxinformslots = 5
informmask = True
requestmask = True
informcountaccepted = 4
byemask = True

###### Dialogue Manager parameters ######

## Uncomment for DQN policy ##
[policy]
policydir = _env6_madqn_tau_nodrop_sort_noinco_nlb_3232_128128_policies
belieftype = focus
useconfreq = False
learning = True
policytype = hack
startwithhello = False
inpolicyfile = auto
outpolicyfile = auto

[dqnpolicy]
maxiter = 4000
gamma = 0.99
learning_rate = 0.001
tau = 0.02
replay_type = vanilla
minibatch_size = 64
capacity = 6000
exploration_type = e-greedy
episodeNum= 0.0
epsilon_start = 0.3
epsilon_end = 0.0
n_in = 636
features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]
max_k = 5
learning_algorithm = dqn
architecture = vanilla
nature_mode = False
madqn_hidden_layers = 2
madqn_local_hidden_units = (32, 32)
madqn_local_dropouts = (0.0, 0.0)
madqn_global_hidden_units = (128, 128)
madqn_global_dropouts = (0.0, 0.0)
madqn_sort_input_vec = True
madqn_input_comm = False
madqn_non_local_mode = True
madqn_block_mode = True
training_frequency = 2
n_samples = 1
stddev_var_mu = 0.01
stddev_var_logsigma = 0.01
mean_log_sigma = 0.000001
sigma_prior = 1.5
alpha =0.85
alpha_divergence =False
sigma_eps = 0.01
delta = 1.0
beta = 0.95
is_threshold = 5.0
train_iters_per_episode = 1

###### Evaluation parameters ######

[eval]
rewardvenuerecommended=0
penaliseallturns = True
wrongvenuepenalty = 0
notmentionedvaluepenalty = 0      
successmeasure = objective 
successreward = 20

